{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daniel Bairamian 260669560 \n",
    "## ECSE 415 Computer Vision\n",
    "### Assignment 3 Part One"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation - K-means clustering and Expectation Maximization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "from skimage import color\n",
    "from random import randint\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import multivariate_normal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some constants\n",
    "EPSILON = 1e-8 # Used to prevent divisions by 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flattens a 2D image into a 1D image\n",
    "# Takes a 3D numpy array and returns a 2D numpy array\n",
    "def flatten_image(img):\n",
    "    np.testing.assert_equal(len(img.shape), 3)\n",
    "    return img.reshape(img.shape[0]*img.shape[1],img.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to show images\n",
    "def show_images(images, cols , titles):\n",
    "    \n",
    "    n_images = len(images)\n",
    "    fig = plt.figure()\n",
    "    for n, (image, title) in enumerate(zip(images, titles)):\n",
    "        a = fig.add_subplot(cols, np.ceil(n_images/float(cols)), n + 1)\n",
    "        plt.xticks([]), plt.yticks([])\n",
    "        plt.imshow(image, cmap=\"gray\")\n",
    "        a.set_title(title)\n",
    "    fig.set_size_inches(np.array(fig.get_size_inches()) * n_images)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Color an image\n",
    "# loop through the image twice to get the average color of a label\n",
    "# then reapply the average color of the label to the class\n",
    "def K_color_image(img, labs, k):\n",
    "    \n",
    "    original_shape = img.shape\n",
    "    temp_image = copy.deepcopy(img)\n",
    "    temp_label = copy.deepcopy(labs)\n",
    "    img_flat = flatten_image(temp_image)\n",
    "    \n",
    "    # make this function work in both RGB and Greyscale\n",
    "    avg_color   = np.zeros([k, img.shape[2]])\n",
    "    class_count = np.zeros([k, 1]) \n",
    "    \n",
    "    \n",
    "    for i, (img_px, label) in enumerate (zip(img_flat, temp_label)):\n",
    "        # moving average of label colour\n",
    "        avg_color[label]  = np.divide( (avg_color[label]*class_count[label]) + img_px,  class_count[label] + 1) \n",
    "        class_count[label]+= 1\n",
    "    \n",
    "    for i, (img_px, label) in enumerate (zip(img_flat, temp_label)):\n",
    "        img_flat[i] = avg_color[label]\n",
    "    \n",
    "    \n",
    "    return img_flat.reshape(original_shape[0], original_shape[1], original_shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_means_cluster(data, k, iter_count):\n",
    "    \n",
    "    labels_list = []\n",
    "    \n",
    "    # Randomly assign a label\n",
    "    labels = np.random.randint(low=0, high=k, size=data.shape[0])\n",
    "    \n",
    "    # Randomly assign centers between minimum and maximum values of the data\n",
    "    centers = np.random.uniform(low=0., high=1., size=(k, data.shape[1]))\n",
    "    centers = centers * (np.max(data, axis=0) - np.min(data, axis=0)) + np.min(data, axis=0)\n",
    "\n",
    "    for i in range(0, iter_count):\n",
    "        # save the first 5 iteration labels\n",
    "        if i < 5:\n",
    "            labels_list.append(labels)\n",
    "        # compute the distances\n",
    "        distances = np.array([np.linalg.norm(data - cen, axis=1) for cen in centers])\n",
    "        # assign the label to the closest distance\n",
    "        lables_new = np.argmin(distances, axis=0)\n",
    "\n",
    "        if (labels == lables_new).all():\n",
    "            # If labels didn't change, we're done, break\n",
    "            labels = lables_new\n",
    "            break\n",
    "        else:\n",
    "            labels = lables_new\n",
    "            for c in range(k):\n",
    "                # prevent taking the mean of empty classes\n",
    "                # occurs when a class doesn't have any labels associated with it\n",
    "                # in that case don't update the center since there are non\n",
    "                # this means the center isn't updated, but is still there\n",
    "                \n",
    "                # maybe a more accurate approach would be to tag the class that doesn't have points associated to it\n",
    "                # and simply remove that class (i.e. decrement K)\n",
    "                # but this seems to be working fine :) \n",
    "                if(len(data[labels==c])!=0):\n",
    "                    centers[c] = np.mean(data[labels == c], axis=0)\n",
    "                else:\n",
    "                    continue\n",
    "    \n",
    "    # save the last label\n",
    "    labels_list.append(labels)\n",
    "    return labels_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_result(image, k, label_array):\n",
    "    \n",
    "    titles = []\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    for i, label_list in enumerate(label_array):\n",
    "        seg_output = K_color_image(image, label_list, k)\n",
    "        images.append(seg_output)\n",
    "        if i != len(label_array)-1:\n",
    "            titles.append(\"Iteration: \" + str(i+1))\n",
    "        else:\n",
    "            titles.append(\"Final Iteration\")\n",
    "        \n",
    "    show_images(images, len(images), titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load image\n",
    "img        = cv2.imread(\"flower.jpg\")\n",
    "img_rgb    = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "img_gscale = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# repeat grayscale image 3 times to make it a 3 dimentional matrix\n",
    "# doing so it uses the same code as the RGB image for K means and K colouring\n",
    "img_gscale_reshaped = np.repeat(img_gscale[:, :, np.newaxis], 3, axis=2)\n",
    "\n",
    "original_images = [img_rgb, img_gscale, img_gscale_reshaped]\n",
    "original_titles = [\"Original Image RGB\", \"Original Image Gray Scale\", \"Gray Scale Reshaped\"]\n",
    "\n",
    "show_images(original_images, len(original_images)/3, original_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten input images\n",
    "img_rgb_flat = flatten_image(img_rgb)\n",
    "img_gs_flat = flatten_image(img_gscale_reshaped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RGB Image K = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "k      = 2\n",
    "n_iter = 1000\n",
    "\n",
    "K_means_result = k_means_cluster(img_rgb_flat, k, n_iter)\n",
    "show_result(img_rgb, k, K_means_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RGB Image K = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "k      = 3\n",
    "n_iter = 1000\n",
    "\n",
    "K_means_result = k_means_cluster(img_rgb_flat, k, n_iter)\n",
    "show_result(img_rgb, k, K_means_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grayscale Image K = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "k      = 2\n",
    "n_iter = 1000\n",
    "\n",
    "K_means_result = k_means_cluster(img_gs_flat, k, n_iter)\n",
    "show_result(img_gscale_reshaped, k, K_means_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grayscale Image K = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "k      = 3\n",
    "n_iter = 1000\n",
    "\n",
    "K_means_result = k_means_cluster(img_gs_flat, k, n_iter)\n",
    "show_result(img_gscale_reshaped, k, K_means_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The segmentation map of the RGB image was better than the grayscaled version. This is because the grayscale image has strictly less information to work with than the RGB image. The RGB image has 3 dimensions to use as information to determine which cluster a data point belongs to, whereas the grayscale image only has 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expectation Maximization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def E_Step(k, img, clusters_alpha, clusters_mean, clusters_std, verbose):\n",
    "   \n",
    "    # get the height and width of the image\n",
    "    height, width, _ = img.shape\n",
    "    \n",
    "    # initalize an empty list of clusters\n",
    "    clusters = np.zeros((height, width), np.int64)\n",
    "    # initialize an empty list of multivariate gaussians\n",
    "    cluster_gaussians_list = []\n",
    "    \n",
    "    # create gaussians for each class\n",
    "    for i in range(0, k):\n",
    "        try:\n",
    "            cluster_gaussians_list.append( multivariate_normal(clusters_mean[i], clusters_std[i]) )\n",
    "        except:\n",
    "            # the multivariate normal sometimes fails to execute. the standard deviation matrix is singular\n",
    "            # I believe this happens when a class cannot have points associated to, and so it becomes empty\n",
    "            # and since no points are associated to it, the matrix becomes 0. And Zero matrices are not invertible hence singular\n",
    "            \n",
    "            # To deal with that, i just set the mean to be 0, and the standard deviation to be all ones.\n",
    "            # This might not be best approach to this problem but at least this makes it so the code doesn't crash\n",
    "            \n",
    "            # A better alternative would probably be to tag the class that doesn't have labels associated to it \n",
    "            # So we can exclude it from any computation so long as it doesn't have labels associated to it\n",
    "            # but honestly this would take too much time for little to no added value.\n",
    "            \n",
    "            # Note: Instead of making the mean 0, i instead make it an array of EPSILONs, which is a tiny constant i defined on top\n",
    "            # this is just so that no divisions by zeros occur\n",
    "            if verbose:\n",
    "                print(\"Cannot calculate Multivariate gaussian for class: \", k)\n",
    "                \n",
    "            cluster_gaussians_list.append(multivariate_normal( np.ones(clusters_mean[i].shape, np.int64)*EPSILON, np.ones(clusters_std[i].shape, np.int64)) )   \n",
    "    \n",
    "    for h in range(0, height):\n",
    "        for w in range(0, width):\n",
    "            \n",
    "            proba_results = []\n",
    "            proba_sum = 0\n",
    "            # calculate the probability of each cluster\n",
    "            for i in range (0, k):\n",
    "                gauss_k = clusters_alpha[i] * cluster_gaussians_list[i].pdf(img[h, w, :])\n",
    "                proba_results.append(gauss_k)\n",
    "                proba_sum += gauss_k\n",
    "                \n",
    "            for i in range (0, k):\n",
    "                # Added epsilon to prevent division by zero when class doesn't have labels\n",
    "                proba_results[i] /= proba_sum + EPSILON\n",
    "            # assign a cluster to each point according to the calculated probabilty\n",
    "            clusters[h, w] = np.argmax(proba_results)\n",
    "            \n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def M_Step(k, img, clusters):\n",
    "    \n",
    "    # get height, width and depth of the image\n",
    "    height, width, depth = img.shape\n",
    "    \n",
    "    # initialize empty arrays of updated arrays\n",
    "    clusters_alpha_new = np.zeros((k), np.float64)\n",
    "    clusters_mean_new = np.zeros((k, depth), np.float64)\n",
    "    clusters_std_new = np.zeros((k, depth), np.float64)\n",
    "\n",
    "    # Compute the clusters alpha and mean\n",
    "    for h in range(0, height):\n",
    "        for w in range(0, width):\n",
    "            clusters_alpha_new[clusters[h, w]] += 1\n",
    "            clusters_mean_new[clusters[h, w], :] += img[h, w, :]\n",
    "\n",
    "\n",
    "    # Compute the clusters standard deviation\n",
    "    for i in range(k):\n",
    "        # Added epsilon to prevent division by zero when class doesn't have labels\n",
    "        clusters_mean_new[i] /= clusters_alpha_new[i] + EPSILON\n",
    "        for h in range(height):\n",
    "            for w in range(width):\n",
    "                if(clusters[h, w] == i):\n",
    "                    clusters_std_new[i] +=  np.dot(img[h, w, :] - clusters_mean_new[i, :], (img[h, w, :] - clusters_mean_new[i, :]).T)\n",
    "                    \n",
    "\n",
    "    for i in range(k):\n",
    "        # Added epsilon to prevent division by zero when class doesn't have labels\n",
    "        clusters_std_new[i] /= (clusters_alpha_new[i] - 1) + EPSILON \n",
    "        clusters_std_new[i] = np.sqrt(clusters_std_new[i])\n",
    "    clusters_alpha_new /= (height * width)\n",
    "    \n",
    "    return clusters_alpha_new, clusters_mean_new, clusters_std_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EM_Initialize(k, img):\n",
    "    \n",
    "    # get height, width, dpeth\n",
    "    height, width, depth = img.shape\n",
    "    \n",
    "    # initialize empty arrays for \n",
    "    # alpha, mean and standard deviation\n",
    "    clusters = np.zeros((height, width), np.int64)\n",
    "    clusters_alpha = np.zeros((k), np.float64)\n",
    "    clusters_mean = np.zeros((k, depth), np.float64)\n",
    "    clusters_std = np.zeros((k, depth), np.float64)\n",
    "    \n",
    "    # give a random cluster for each point\n",
    "    for h in range(0, height):\n",
    "        for w in range(0, width):\n",
    "            cluster = np.random.randint(k, size=1)\n",
    "            cluster = cluster[0]\n",
    "            clusters[h, w] = cluster\n",
    "            clusters_alpha[cluster] += 1\n",
    "            clusters_mean[cluster, :] += img[h, w, :]        \n",
    "\n",
    "    # compute the standard deviation\n",
    "    for i in range(0, k):\n",
    "         # compute the mean\n",
    "        clusters_mean[i] /= clusters_alpha[i]\n",
    "        for h in range(0, height):\n",
    "            for w in range(0, width):\n",
    "                if(clusters[h, w] == i):\n",
    "                    clusters_std[i] +=  np.dot(img[h, w, :] - clusters_mean[i, :], (img[h, w, :] - clusters_mean[i, :]).T)\n",
    "\n",
    "    for i in range(0, k):\n",
    "        clusters_std[i] /= (clusters_alpha[i] - 1) + EPSILON \n",
    "        clusters_std[i] = np.sqrt(clusters_std[i])\n",
    "    clusters_alpha /= (height * width) + EPSILON\n",
    "\n",
    "    return clusters_alpha, clusters_mean, clusters_std, clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EM(k, img, iter_count, verbose = False):\n",
    "    total_labels = []\n",
    "    # initialize all the data we need\n",
    "    clusters_alpha,  clusters_mean, clusters_std, clusters = EM_Initialize(k, img)\n",
    "    \n",
    "    # save the first 5 iterations\n",
    "    for i in range(iter_count):\n",
    "        if i < 5:\n",
    "            total_labels.append(clusters.flatten())\n",
    "        if verbose:\n",
    "            print(\"Iteration: \", i+1, \"/\", iter_count)\n",
    "        \n",
    "        # E step\n",
    "        clusters = E_Step(k, img, clusters_alpha, clusters_mean, clusters_std, verbose)\n",
    "        # M step\n",
    "        clusters_alpha, clusters_mean, clusters_std = M_Step(k, img, clusters)\n",
    "    \n",
    "    # save the last iteration\n",
    "    total_labels.append(clusters.flatten())\n",
    "    return total_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RGB Image K = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "k      = 2\n",
    "n_iter = 15\n",
    "\n",
    "EM_results = EM(k, img_rgb.copy(), n_iter, verbose=True)\n",
    "show_result(img_rgb, k, EM_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RGB Image K = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "k      = 3\n",
    "n_iter = 15\n",
    "\n",
    "EM_results = EM(k, img_rgb.copy(), n_iter, verbose=True)\n",
    "show_result(img_rgb, k, EM_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grayscale Image K = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "k      = 2\n",
    "n_iter = 15\n",
    "\n",
    "EM_results = EM(k, img_gscale_reshaped.copy(), n_iter, verbose=True)\n",
    "show_result(img_gscale_reshaped, k, EM_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grayscale Image K = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "k      = 3\n",
    "n_iter = 15\n",
    "\n",
    "EM_results = EM(k, img_gscale_reshaped.copy(), n_iter, verbose=True)\n",
    "show_result(img_gscale_reshaped, k, EM_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just like the K means algorithm, the EM segmentation also performs better on the RGB image than on the grayscale image, and this for the same reasons stated above.\n",
    "\n",
    "### An additional issue the grayscale version faces that is less likely to occur in the RGB version is singular covariance matrices. Since there is less information to work with, the algorithm might converge to a steady state where it is only able to assign points to C classes, where C is strictly less than the original K picked. (C < K)\n",
    "\n",
    "### This then means that during the training loop, the covariance matrix and mean array for some class will become 0, which could cause issues if it is not properly dealt with. (matrix cannot be inverted).\n",
    "\n",
    "### This issue can also occur in the RGB case, but since we have more information to work with, this occurance of a class being empty is less likely"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datapoints Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.float32(\n",
    "    np.vstack((\n",
    "        np.random.normal(\n",
    "            loc = np.array([5,5]),\n",
    "            scale = np.array([3,2]),\n",
    "            size = (1000, 2)),\n",
    "        np.random.normal(\n",
    "            loc = np.array([-5, -5]),\n",
    "            scale = np.array([5, 1]),\n",
    "            size = (1000, 2)\n",
    "        ))))\n",
    "\n",
    "\n",
    "# EM expects 2D data, so just giving data arbitrairy rectangular shape\n",
    "# the reshape shouldn't affect the results since there are no Kernels used\n",
    "# and no convolutions. Meaning there is no spatial information being taken into consideration when segmenting\n",
    "# which is why this reshape shouldn't affect the final segmentation, it just makes my life easier :)\n",
    "\n",
    "data2 = data.reshape(40, 50, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Means "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_count = 100000\n",
    "k = 2\n",
    "\n",
    "label_cluster_KM = k_means_cluster(data, k, iter_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can easily split the data into different clusters depending on their labels\n",
    "A = data[label_cluster_KM[-1].ravel()==0]\n",
    "B = data[label_cluster_KM[-1].ravel()==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(121)\n",
    "plt.plot(data[:,0], data[:,1],\"ko\")\n",
    "plt.title(\"Data points\")\n",
    "plt.subplot(122)\n",
    "plt.scatter(A[:,0], A[:,1], color='b')\n",
    "plt.scatter(B[:,0], B[:,1], color='r')\n",
    "\n",
    "plt.title(\"K Means: Data points - Labeled\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_count = 200\n",
    "k = 2\n",
    "\n",
    "labels_EM = EM(k, data2.copy(), iter_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can easily split the data into different clusters depending on their labels\n",
    "A = data[labels_EM[-1].flatten().ravel()==0]\n",
    "B = data[labels_EM[-1].flatten().ravel()==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(121)\n",
    "plt.plot(data[:,0], data[:,1],\"ko\")\n",
    "plt.title(\"Data points\")\n",
    "plt.subplot(122)\n",
    "plt.scatter(A[:,0], A[:,1], color='b')\n",
    "plt.scatter(B[:,0], B[:,1], color='r')\n",
    "\n",
    "plt.title(\"EM: Data points - Labeled\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From the results above, we can see that EM performs better than K means at classifyingour generated data. This is to be expected since the data points were generated with 2-D gaussians. \n",
    "\n",
    "### The reason why this is to be expected, is because K means uses eucledian distance to jugde class membership of a datapoint. However, it does not take variance into consideration. Even if a point is closer to a center in eucledian distance, it may be very unlikely to actually belong to that center's class, since the variance of that class is very small in the direction of that datapoint\n",
    "\n",
    "### EM is better at classifying our gaussian data points, because it uses probabilities rather than eucledian distance to judge class membership. In layman's terms, it uses probabilities to \"warp\" the space, so that class centers appear closer or further than they would in eucledian space. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalized graph-cut and Mean-Shift segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalized Graph Cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import segmentation, color\n",
    "from skimage.future import graph\n",
    "from skimage.segmentation import quickshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load image\n",
    "img        = cv2.imread(\"flower.jpg\")\n",
    "img_rgb    = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "original_images = [img_rgb]\n",
    "original_titles = [\"Original Image RGB\"]\n",
    "\n",
    "show_images(original_images, len(original_images), original_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "titles = []\n",
    "\n",
    "compactness_values = [1, 10, 50, 100]\n",
    "n_segments_values  = [50, 200, 500]\n",
    "thresh_values      = [0.005, 0.05, 0.5]\n",
    "\n",
    "iter_count = 0\n",
    "total_iters= len(compactness_values)*len(n_segments_values)*len(thresh_values)\n",
    "\n",
    "for comp in compactness_values:\n",
    "    for seg in n_segments_values:\n",
    "        for threshold in thresh_values:\n",
    "            iter_count += 1\n",
    "            print(\"Iteration: \", iter_count, \"/\", total_iters)\n",
    "            # K means segmenration to generate super pixels\n",
    "            labels1 = segmentation.slic(img_rgb, compactness=comp, n_segments=seg)\n",
    "            # Region adjacency graph\n",
    "            g = graph.rag_mean_color(img_rgb, labels1, mode='similarity')\n",
    "            labels2 = graph.cut_normalized(labels1, g, thresh=threshold)\n",
    "            # apply graph cut\n",
    "            out2 = color.label2rgb(labels2, img_rgb, kind='avg')\n",
    "    \n",
    "            # append output to result list\n",
    "            images.append(out2)\n",
    "            titles.append(\"Compactness: \"+ str(comp)+ \", N-Segments: \" + str(seg) +\", Trheshold: \" + str(threshold)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(images, len(images), titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The n_segments parameter seem to affect how many classes we think there are, similar to K in our K means and EM.\n",
    "\n",
    "### Compactness seems to have an effect on depth, and the amount of details preserved in the background. A higher value of compactness seems to give more importance to the background, and lower values seem to give less importance to the background.\n",
    "\n",
    "\n",
    "### The threshold parameter seems to have an effect on the sub-division of a class. The value of the threshold has an effect on how the region within a class is divided. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Shift Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "titles = []\n",
    "\n",
    "kernel_values    = [5, 15, 30, 60]\n",
    "max_dist_values  = [5, 20, 40, 60, 100]\n",
    "\n",
    "iter_count = 0\n",
    "total_iters= len(kernel_values)*len(max_dist_values)\n",
    "\n",
    "for kernel in kernel_values:\n",
    "    for dist in max_dist_values:\n",
    "        iter_count += 1\n",
    "        print(\"Iteration: \", iter_count, \"/\", total_iters)\n",
    "        # apply meanshift\n",
    "        label = quickshift(img_rgb, kernel_size=kernel, max_dist=dist)\n",
    "        out = color.label2rgb(label, img_rgb, kind='avg')\n",
    "        images.append(out)\n",
    "        titles.append(\"Kernel: \"+ str(kernel)+ \", Distance: \" + str(dist)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "show_images(images, len(images), titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The kernel size seems to have an effect on how many different clusters we have. It controls the size of a kernel, which then must mean that the higher this kernel, the more information it treats at a given time, so the *fewer* the classes we will have. Essentially this parameter is like K in K means and EM, only it seems to inversely proportional in this case. (The higher the number the fewer the classes)\n",
    "\n",
    "\n",
    "### The max distance also seems to have an effect on how many different clusters we have. It must be analogous to the K means distance, where it  represent some threshold distance to determine if a datapoint is a member of a class"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
